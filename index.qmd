---
title: "Week 12"
title-block-banner: true
title-block-style: default
execute:
  freeze: true
  cache: true
format:
  html: # comment this line to get pdf
  # pdf: 
    fig-width: 7
    fig-height: 7
---

```{r}
#| tags: []
#| vscode: {languageId: r}
dir <- "~/work/courses/stat380/weeks/week-12/"
# renv::activate(dir)
```


#### Packages we will require this week

```{r warnings=F, message=F, results='hide'}
#| tags: []
#| vscode: {languageId: r}
packages <- c(
    # Old packages
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "repr",
    "tidyverse",
    "kableExtra",
    "IRdisplay",
    "car",
    "corrplot",
    # NEW
    "torch",
    "torchvision",
    "luz",
    # Dimension reduction
    "dimRed",
    "RSpectra"
)

# renv::install(packages)
sapply(packages, require, character.only = TRUE)
```

---

# Tue, Apr 12

### Agenda:

1. Real-world neural network classification
1. Dataloaders
1. Torch for image classification

<br><br><br>

## Titanic

```{r}
#| tags: []
#| vscode: {languageId: r}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read_csv(url) %>%
    mutate_if(\(x) is.character(x), as.factor) %>%
    mutate(y = Survived) %>%
    select(-c(Name, Survived)) %>%
    (\(x) {
        names(x) <- tolower(names(x))
        x
    })

df %>% head
```

## Breast Cancer Prediction

```{r}
#| tags: []
#| vscode: {languageId: r}
# url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

# col_names <- c("id", "diagnosis", paste0("feat", 1:30))

# df <- read_csv(
#         url, col_names, col_types = cols()
#     ) %>% 
#     select(-id) %>% 
#     mutate(y = ifelse(diagnosis == "M", 1, 0)) %>%
#     select(-diagnosis)


# df %>% head
```

### Train/Test Split

```{r}
#| tags: []
#| vscode: {languageId: r}
k <- 5

test_ind <- sample(
    1:nrow(df), 
    floor(nrow(df) / k),
    replace=FALSE
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]

nrow(df_train) + nrow(df_test) == nrow(df)
```

### Benchmark with Logistic Regression

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_glm <- glm(
    y ~ ., 
    df_train %>% mutate_at("y", factor), 
    family = binomial()
)

glm_test <- predict(
    fit_glm, 
    df_test,
    output = "response"
)

glm_preds <- ifelse(glm_test > 0.5, 1, 0)
table(glm_preds, df_test$y)
```

### Neural Net Model

```{r}
#| tags: []
#| vscode: {languageId: r}
NNet <- nn_module(
  initialize = function(p, q1, q2, q3) {  
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$output <- nn_linear(q3, 1)
    self$activation <- nn_relu()
    self$sigmoid <- nn_sigmoid()
  },
    
  forward = function(x) {
    x %>% 
      self$hidden1() %>% self$activation() %>% 
      self$hidden2() %>% self$activation() %>% 
      self$hidden3() %>% self$activation() %>% 
      self$output() %>% self$sigmoid()
  }
)
```

### Fit using Luz

```{r}
#| tags: []
#| vscode: {languageId: r}
M <- model.matrix(y ~ 0 + ., data = df_train)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_nn <- NNet %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_bce_loss(),
        optimizer = optim_adam, 
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>% 
    #
    # Set the hyperparameters
    #
    set_hparams(p=ncol(M), q1=256, q2=128, q3=64) %>% 
    set_opt_hparams(lr=0.005) %>% 
    #
    # Fit the model
    #
    fit(
        data = list(
            model.matrix(y ~ 0 + ., data = df_train),
            df_train %>% select(y) %>% as.matrix
        ),
        valid_data = list(
            model.matrix(y ~ 0 + ., data = df_test),
            df_test %>% select(y) %>% as.matrix
        ),
        epochs = 50, 
        verbose = TRUE
    )
```

```{r}
#| tags: []
#| vscode: {languageId: r}
plot(fit_nn)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
nn_test <- predict(
    fit_nn, 
    model.matrix(y ~ . - 1, data = df_test)
)
# nn_test
nn_preds <- ifelse(nn_test > 0.5, 1, 0)

table(nn_preds, df_test$y)
mean(nn_preds == df_test$y)
```

```{r}
#| vscode: {languageId: r}
table(glm_preds, df_test$y)
mean(glm_preds == df_test$y)
```

<br><br><br><br>

---

### DataLoaders

* Dataloaders are a key component in the machine learning pipeline.

* They handle loading and preprocessing data in a way that is efficient for training and evaluating models.

* Dataloaders make it easy to work with large datasets by loading the data in smaller chunks (called **batches**) and applying transformations _on-the-fly_.


##### Why use Dataloaders?

> * **Efficient memory management:** loading data in smaller chunks reduces memory usage.
>
> * **Parallelism:**  supports asynchronous data loading for faster processing.
>
> * **Preprocessing:**  apply data transformations on-the-fly during training and evaluation.
>
> * **Flexibility:**  easily switch between different datasets or preprocessing steps.
>
> * **Standardization:**  consistent data format across various machine learning projects.
>

```{r}
#| tags: []
#| vscode: {languageId: r}
# ?dataloader
```

```{r}
#| tags: []
#| vscode: {languageId: r}
transform <- function(x) x %>% 
    torch_tensor() %>% 
    torch_flatten() %>% 
    torch_div(255)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
dir <- "./mnist"

train_ds <- mnist_dataset(
    root = dir,
    train = TRUE,
    download = TRUE,
    transform = transform
)
test_ds <- mnist_dataset(
    root = dir,
    train = FALSE,
    download = TRUE,
    transform = transform
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
typeof(train_ds)
length(train_ds)
train_ds$data[42000, ,]
```

```{r, fig.width: 7, fig.height: 7}
#| tags: []
#| vscode: {languageId: r}

i <- sample(1:length(train_ds), 1)
x <- train_ds$data[i, ,] %>% t

image(x[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000), main = train_ds$targets[i]-1 )
```

```{r, fig.width: 10, fig.height: 10}
#| tags: []
#| vscode: {languageId: r}
par(mfrow=c(3,3))

for(iter in 1:9){
    i <- sample(1:length(train_ds), 1)
    x <- train_ds$data[i, ,] %>% t
    image(x[1:28, 28:1], useRaster = TRUE, axes = FALSE, col = gray.colors(1000), main = train_ds$targets[i]-1)
}
```

<br><br><br><br>
<br><br><br><br>

---

# Image Classification

```{r}
#| tags: []
#| vscode: {languageId: r}
train_dl <- dataloader(train_ds, batch_size = 1024, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 1024)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
NNet_10 <- nn_module(
  initialize = function(p, q1, q2, q3, o) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$OUTPUT <- nn_linear(q3, o)
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() %>%
      self$activation() %>%
      self$OUTPUT()
  }
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_nn <- NNet_10 %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_cross_entropy_loss(),
        optimizer = optim_adam,
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>%
    #
    # Set the hyperparameters
    #
    set_hparams(p=28*28, q1=256, q2=128, q3=64, o=10) %>% 
    #
    # Fit the model
    #
    fit(
        epochs = 10,
        data = train_dl,
        # valid_data = test_dl,
        verbose=TRUE
    )
```

```{r}
#| tags: []
#| vscode: {languageId: r}
NN10_preds <- fit_nn %>% 
  predict(test_ds) %>% 
  torch_argmax(dim = 2) %>%
  as_array()
```


::: {.callout-tip}
#### Accuracy

```{r}
mean(NN10_preds == test_ds$targets)
```
:::

::: {.callout-tip}
## Confusion matrix

```{r}
table(NN10_preds - 1, test_ds$targets - 1)
```


```{r}
caret::confusionMatrix(
  (NN10_preds - 1) %>% as.factor, 
  (test_ds$targets - 1) %>% as.factor
)
```
:::

```{r}
#| tags: []
options(repr.plot.width = 10, repr.plot.height = 10)
par(mfrow=c(3,3))

for(iter in 1:9){
    i <- sample(1:length(test_ds), 1)
    x <- test_ds$data[i, ,] %>% t
    image(x[1:28, 28:1], useRaster = TRUE, axes = FALSE, col = gray.colors(1000), main = paste("predicted =", NN10_preds[i] - 1))
}
```

<br><br><br><br>
<br><br><br><br>
<br><br><br><br>

---

# Thu, Apr 13


## Supervised learning

For a majority of this course we have focused on **supervised learning** where we have access to **labelled data** i.e., we are given access to the _covariates and the responses_

<br><br><br>

$$
\begin{aligned}
\text{observation}\ 1: &\quad (X_{1, 1}, X_{2, 1}, \dots X_{p, 1}, y_1)\\
\text{observation}\ 2: &\quad (X_{1, 2}, X_{2, 2}, \dots X_{p, 2}, y_2)\\
\vdots\quad & \quad\quad\quad\vdots\\ 
\text{observation}\ n: &\quad (X_{1, n}, X_{2, n}, \dots X_{p, n}, y_n)
\end{aligned}
$$

<br><br><br>

Our **goal** has been to:

* Predict $y$ using $X_1, X_2, \dots X_p$
* Understand how each $X_i$ influences the response $y$

## Unsupervised learning

In unsupervised learning we **DON'T** have access to the labelled data, i.e., we are only given:

<br><br><br>

$$
\begin{aligned}
\text{observation}\ 1: &\quad (X_{1, 1}, X_{2, 1}, \dots X_{p, 1})\\
\text{observation}\ 2: &\quad (X_{1, 2}, X_{2, 2}, \dots X_{p, 2})\\
\vdots\quad & \quad\quad\quad\vdots\\ 
\text{observation}\ n: &\quad (X_{1, n}, X_{2, n}, \dots X_{p, n})
\end{aligned}
$$

<br><br><br>

Our **goal** here is:

* To understand the relationship between $X_1, X_2, \dots X_p$

    > * **dimension reduction**: 
    > 
    > Can we discover subgroups of variables $X_1, X_2, \dots X_p$ which behave similarly?
    
    > * **clustering**:
    >
    > Can we discover subgroups of observations $1, 2, \dots n$ which are similar?
    
<br><br><br>

#### Why unsupervised learning?

It is always easier to obtain unlabeled data as oppposed to labeled data (which require someone or something to actually assign the proper responses $y_1, y_2, \dots y_n$)

In statistics and data science, there are a multitude of different methods which have been proposed to tackle each of the two problems. They include:

* **Dimension reduction**:
    * Principal component analysis
    * Uniform Manifold Approximation (UMAP)
    * t-Stochastic Neighbor embedding (t-SNE)
    * ...
    
* **Clustering**:
    * k-means clustering
    * Hierarchical clustering
    * Topological clustering
    * Laplacian eigenmaps
    * ...
    
This is one of the most exciting parts of data-science

---


## Principal Component Analysis (PCA)

Given variables $(X_1, X_2, \dots X_p)$, PCA produces a low-dimensional representation of the dataset, i.e., 

<br><br>

$$
\begin{aligned}
\text{observation}\ 1: &\quad (X_{1, 1}, X_{2, 1}, \dots X_{p, 1}) \longrightarrow (Z_{1, 1}, Z_{2, 1})\\
\text{observation}\ 2: &\quad (X_{1, 2}, X_{2, 2}, \dots X_{p, 2}) \longrightarrow (Z_{1, 2}, Z_{2, 2})\\
\vdots\quad & \quad\quad\quad\vdots\\ 
\text{observation}\ n: &\quad (X_{1, n}, X_{2, n}, \dots X_{p, n}) \longrightarrow (Z_{1, n}, Z_{2, n})
\end{aligned}
$$

<br><br>

It tries to create variables $(Z_1, Z_2, \dots Z_q)$ for $q < p$ such that:

1. $q \ll p$
1. $(Z_1, Z_2, \dots Z_q)$ contains _roughly_ the same information as $(X_1, X_2, \dots X_p)$


<br><br>
<br><br>

---

#### How does PCA achieve this?

<br><br>

The `Julia` notebook [here](jl/pca.html) illustrates this process in $2d$ and $3d$.


##### Step 1:

The **first principal component** $Z_1$ is the _normalized_ linear combination of the features:

<br><br>

$$
Z_1 = v_{11} X_1 + v_{21} X_2 + \dots v_{p1} X_p
$$

<br><br>

such that: 

* $Z_1$ has the largest possible variance
* $\sum_{i=1}^p v^2_{p, i} = 1$

<br><br>


> #### Note:
> $V_1 = (v_{11}, v_{21}, \dots v_{p1})$ are called the **factor loadings** of the first principal component.

<br><br>

---

<br><br>


##### Step 2:

The **second principal component** $Z_2$ is the _normalized_ linear combination of the features:

<br><br>

$$
Z_2 = v_{12} X_1 + v_{22} X_2 + \dots v_{p2} X_p
$$

<br><br>

such that: 

* $V_2 \perp V_1$
* $Z_2$ has the largest possible variance
* $\sum_{i=1}^p v^2_{p, 2} = 1$

<br><br>

---

$$
\begin{aligned}
\vdots
\\
\vdots
\end{aligned}
$$

---

##### Step q:

The **$q$th principal component** $Z_q$ is the _normalized_ linear combination of the features:

<br><br>

$$
Z_2 = v_{12} X_1 + v_{22} X_2 + \dots v_{p2} X_p
$$

<br><br>

such that: 

* $Z_q$ has the largest possible variance
* $V_q \perp \text{span}(V_1, V_2, \dots, V_{q-1})$
* $\sum_{i=1}^p v^2_{p, 2} = 1$

<br><br>



<br><br>

---

# Tue, Apr 18

## Example in `R`

In R, we can use the built-in function prcomp() to perform PCA.


```{r}
data <- tibble(
  x1 = rnorm(100, mean = 0, sd = 1),
  x2 = x1 + rnorm(100, mean = 0, sd = 0.1),
  x3 = x1 + rnorm(100, mean = 0, sd = 0.1)
)

head(data) %>% knitr::kable()
```

```{r}
pca <- princomp(data, cor = TRUE)
summary(pca)
```

```{r}
screeplot(pca, type="l")
```

```{r}
par(mfrow=c(1, 2))

Z_pca <- predict(pca, data)
plot(data)
plot(Z_pca)
```

```{r}
pca$loadings
```


---


### Interpretation of principal components


```{r}
set.seed(42)

n <- 500
science <- rnorm(n, mean = 60, sd = 10)
humanities <- rnorm(n, mean = 80, sd=10)

df <- tibble(
  math = 0.8 * science + rnorm(n, mean = 0, sd = 7),
  physics = 1.0 * science + rnorm(n, mean = 0, sd = 5),
  chemistry = 1.3 * science + rnorm(n, mean = 0, sd = 3),
  history = 0.8 * humanities + rnorm(n, mean = 0, sd = 5),
  geography = 1.0 * humanities + rnorm(n, mean = 0, sd = 10),
  literature = 1.2 * humanities + rnorm(n, mean = 0, sd = 2)
)
```

```{r}
df %>%
    head() %>%
    round(digits = 2) %>%
    knitr::kable()
```

```{r}
plot(df$math, df$physics)
```

```{r}
pca <- princomp(df, cor=TRUE)
summary(pca)
```

```{r}
pca$loadings
```

```{r}
plot(pca, type="l")
```


---

#### Principal component regression

```{r}
df$gpa <- (0.9 * science + 0.5 * humanities + rnorm(n, mean=0, sd=10)) * 4 / 100

df %>% 
    head() %>%
    round(digits=2) %>%
    knitr::kable()
```

```{r}
lm_fit <- lm(gpa ~ ., df)
summary(lm_fit)
```

```{r}
vif(lm_fit) %>% t
```

```{r}
df %>% 
    cor() %>% 
    corrplot(diag=F)
```

```{r}
pca <- princomp(df %>% select(-gpa), cor=TRUE)
screeplot(pca)
```

```{r}
Z <- predict(pca, df)

df_pca <- Z %>% 
    as_tibble %>% 
    select(Comp.1, Comp.2) %>% 
    mutate(gpa = df$gpa)

head(df_pca) %>% knitr::kable()
```

```{r}
df_pca %>% 
    cor() %>% 
    corrplot(diag=F)
```
    
```{r}
lm_pca <- lm(gpa ~ ., df_pca)
summary(lm_pca)
```

```{r}
vif(lm_pca) %>% t
```

----

# Nonlinear dimension reduction

```{r}
generate_two_spirals <- function(){
  set.seed(42)
  n <- 500
  noise <- 0.05
  t <- (1:n) / n * 4 * pi
  x1 <- t * (sin(t) + rnorm(n, 0, noise))
  x2 <- t * (cos(t) + rnorm(n, 0, noise))
  y  <- t
  return(tibble(x1=x1, x2=x2, y=y))
}
```

```{r}
df <- generate_two_spirals()
head(df)
```

```{r}
ggplot(df) +
    geom_point(aes(x=x1, y=x2, col=y), alpha=0.5) +
    scale_colour_gradient(low="red", high="blue")

```
    
```{r}
pca <- princomp(df[, 1:2], cor=T)
pca$loadings
```

```{r}
df_pca <- predict(pca, df)
head(df_pca)
```

```{r}
ggplot(as_tibble(df_pca)) +
    geom_point(aes(x=Comp.1, y=0, col=df$y), alpha=0.5) +
    scale_colour_gradient(low="red", high="blue")
```

```{r}
library(RDRToolbox)
isomap <- Isomap(df[, 1:2] %>% as.matrix, dims=1)

```

```{r}
ggplot(as_tibble(isomap)) +
    geom_point(aes(x=dim1, y=0, col=df$y)) +
    scale_colour_gradient(low="red", high="blue")
```


---

# Autoencoder


![Autoencoder](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ydTBLRhllAgnqXUO5CSonQ.png)

[image source](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ydTBLRhllAgnqXUO5CSonQ.png)


```{r}
autoencoder <- nn_module(
    initialize = function(p, q1, q2, q3, o) {
    self$encoder <- nn_sequential(
        nn_linear(p, q1), nn_relu(),
        nn_linear(q1, q2), nn_relu(),
        nn_linear(q2, q3), nn_relu(),
        nn_linear(q3, o)
    )
    self$decoder <- nn_sequential(
        nn_linear(o, q3), nn_relu(),
        nn_linear(q3, q2), nn_relu(),
        nn_linear(q2, q1), nn_relu(),
        nn_linear(q1, p)
    )
    },
    forward = function(x) {
    x %>%
        torch_reshape(c(-1, 28 * 28)) %>% 
        self$encoder() %>%
        self$decoder() %>% 
        torch_reshape(c(-1, 28, 28))
    },
    predict = function(x) {
    x %>% 
        torch_reshape(c(-1, 28 * 28)) %>% 
        self$encoder()     
    }
)
```


```{r}
X <- test_ds
inputs <- torch_tensor(X$data * 1.0)
```


```{r}
plot_image = \(x) image(t(x)[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000))
```


#### Original vs. Decoded (at initialization)

```{r}
AE <- autoencoder(p = 28 * 28, q1 = 32, q2 = 16, q3 = 8, o = 2)
```

```{r, plot.width=10, plot.height=5}
par(mfrow=c(4, 2))

set.seed(123)
for(k in 1:4){
    i <- sample(1:10000, 1)
    input <- inputs[i]
    output <- AE(inputs[i:i])[1]

    par(mfrow=c(1, 2))
    plot_image(inputs[i] %>% as_array)
    title("Original")
    
    plot_image(output %>% as_array)
    title("Decoded")
}
```


#### Fitting the autoencoder using `luz`

```{r}
ae_fit <- autoencoder %>%
    setup(
        loss = nn_mse_loss(),
        optimizer = optim_adam
    ) %>%

    set_hparams(
        p=28*28, q1=128, q2=64, q3=32, o=2
    ) %>%
    
    set_opt_hparams(
        lr=1e-3
    ) %>%

    fit(
        data = list(
            inputs, 
            inputs # targets are the same as inputs
        ),
        epochs=30,
        verbose=TRUE,
        dataloader_options = list(
            batch_size = 100, 
            shuffle=TRUE
        ),
        callbacks = list(
            luz_callback_lr_scheduler(
                torch::lr_step, 
                step_size = 10, 
                gamma=1.01
                )
        )
    )

```

```{r}
plot(ae_fit)
```


#### Lower-dimensional Encoding of the Data

```{r}
X_dim2 <- predict(ae_fit, inputs) %>% as_array()
head(X_dim2)
df_ae <- tibble(
    x1 = X_dim2[, 1],
    x2 = X_dim2[, 2],
    y = as.factor(X$targets - 1)
)

ggplot(df_ae) +
    geom_point(aes(x=x1, y=x2, col=y))
```



#### Original vs. Decoded (after fitting)

```{r, plot.width=10, plot.height=5}
par(mfrow=c(4, 2))

set.seed(123)
for(k in 1:4){
    i <- sample(1:10000, 1)
    input <- inputs[i]
    output <- ae_fit$model$forward(inputs[i:i])[1]

    par(mfrow=c(1, 2))
    plot_image(inputs[i] %>% as_array)
    title("Original")
    
    plot_image(output %>% as_array)
    title("Decoded")
}
```